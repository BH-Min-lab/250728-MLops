import os
import numpy as np
import pandas as pd
import joblib
from minio import Minio
from utils import get_data_path, mapping_columns, split_wide_deep_by_type, CATEGORY_MAP, MONTH_MAP

import torch
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
from pathlib import Path

print("Current Working Directory:", os.getcwd())
SYMBOLS = ['Customer', 'Discount', 'Marketing',
           'Onlinesales', 'Tax']  # !!! ìˆ˜ì • ê¸ˆì§€ !!!


class InfoDataset(Dataset):
    def __init__(self, is_training=True, in_columns=None, out_columns=None, data_dir='/opt/airflow/data',
                 new_onlinesales_path=None, minio_endpoint="127.0.0.1:9000",
                 minio_access_key="admin", minio_secret_key="admin",
                 raw_data_bucket="raw-data", new_sales_bucket="new-sales", **kwargs):
        if in_columns is None:
            in_columns = ['ê³ ê°ID', 'ê±°ë˜ID', 'ê±°ë˜ë‚ ì§œ', 'ì œí’ˆID', 'ì œí’ˆì¹´í…Œê³ ë¦¬', 'ìˆ˜ëŸ‰', 'í‰ê· ê¸ˆì•¡', 'ë°°ì†¡ë£Œ', 'ì¿ í°ìƒíƒœ',
                          'ì„±ë³„', 'ê³ ê°ì§€ì—­', 'ê°€ì…ê¸°ê°„', 'GST', 'ì›”', 'ì¿ í°ì½”ë“œ', 'í• ì¸ìœ¨', 'ê±°ë˜ê¸ˆì•¡']
        if out_columns is None:
            out_columns = ['ì œí’ˆì¹´í…Œê³ ë¦¬']

        print("[DEBUG] MinIO endpoint:", minio_endpoint)

        self.minio_client = Minio(
            minio_endpoint,
            access_key=minio_access_key,
            secret_key=minio_secret_key,
            secure=False
        )
        self.raw_data_bucket = raw_data_bucket
        self.new_sales_bucket = new_sales_bucket

        (wide_x, deep_x), self.y = make_features(
            in_columns, out_columns, is_training, data_dir, new_onlinesales_path,
            minio_endpoint, minio_access_key, minio_secret_key, raw_data_bucket, new_sales_bucket
        )
        self.wide_x = torch.from_numpy(wide_x).float()
        self.deep_x = torch.from_numpy(deep_x).float()
        self.y = torch.from_numpy(self.y).float()

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return (self.wide_x[idx], self.deep_x[idx]), self.y[idx]


def merge_data(symbols, data_dir, new_onlinesales_path=None,
               minio_endpoint="127.0.0.1:9000", minio_access_key="admin",
               minio_secret_key="admin", raw_data_bucket="raw-data", new_sales_bucket="new-sales"):
    print("merging raw csv files...")

    # ê¸°ë³¸ 5ì¢… ë¡œë“œ
    dfs = {symbol: pd.read_csv(get_data_path(
        symbol, data_dir, minio_endpoint, minio_access_key, minio_secret_key, raw_data_bucket))
        for symbol in symbols}

    for name, df in dfs.items():
        print(f"[DEBUG] {name} rows: {len(df)}")

    # ìƒˆ ê±°ë˜ CSVê°€ ì§€ì •ëœ ê²½ìš°ì—ë§Œ ì‹œë„
    if new_onlinesales_path:
        client = Minio(minio_endpoint, access_key=minio_access_key,
                       secret_key=minio_secret_key, secure=False)
        fetched = False
        try:
            # ì˜¤ë¸Œì íŠ¸ ì¡´ì¬ í™•ì¸ í›„ ë‹¤ìš´ë¡œë“œ
            client.stat_object(new_sales_bucket, "Onlinesales_new.csv")
            client.fget_object(
                new_sales_bucket, "Onlinesales_new.csv", new_onlinesales_path)
            print(
                f"Downloaded Onlinesales_new.csv from MinIO: {new_sales_bucket}/Onlinesales_new.csv")
            fetched = True
        except Exception as e:
            print(f"[INFO] MinIO ì—ì„œ Onlinesales_new.csv í™•ì¸/ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ë¡œì»¬ì— ìˆìœ¼ë©´ ì‚¬ìš© (MinIO ì‹¤íŒ¨ì‹œ ëŒ€ì²´)
        if fetched or os.path.exists(new_onlinesales_path):
            new_df = pd.read_csv(new_onlinesales_path)
            # ê¸°ì¡´ Onlinesales ì— "ì¶”ê°€"
            dfs['Onlinesales'] = pd.concat(
                [dfs['Onlinesales'], new_df], ignore_index=True)
            print(
                f"[DEBUG] Onlinesales rows after concat: {len(dfs['Onlinesales'])}")
        else:
            print("[INFO] Onlinesales_new.csv ê°€ MinIO/ë¡œì»¬ì— ì—†ìŒ. ìƒˆ ê±°ë˜ ë³‘í•© ìƒëµ.")

    # ë³‘í•© ìˆœì„œ: Onlinesales + Customer â†’ Tax â†’ Discount
    merged = pd.merge(dfs['Onlinesales'], dfs['Customer'], on='ê³ ê°ID')
    merged = pd.merge(merged, dfs['Tax'], on='ì œí’ˆì¹´í…Œê³ ë¦¬')

    # ì œí’ˆì¹´í…Œê³ ë¦¬ ë§¤í•‘
    merged['ì œí’ˆì¹´í…Œê³ ë¦¬'] = merged['ì œí’ˆì¹´í…Œê³ ë¦¬'].map(
        CATEGORY_MAP).fillna(merged['ì œí’ˆì¹´í…Œê³ ë¦¬'])

    # Discountì—ì„œ 'Notebooks' ì œì™¸
    discount_df = dfs['Discount'][dfs['Discount']
                                  ['ì œí’ˆì¹´í…Œê³ ë¦¬'] != 'Notebooks'].copy()

    # ì›” íŒŒìƒ í›„ ë¬¸ìì—´ë¡œ ë§¤í•‘
    merged['ì›”'] = pd.to_datetime(merged['ê±°ë˜ë‚ ì§œ']).dt.month.map(MONTH_MAP)

    # Discount ë³‘í•©
    merged = pd.merge(merged, discount_df, on=['ì œí’ˆì¹´í…Œê³ ë¦¬', 'ì›”'])

    # ê±°ë˜ë‚ ì§œ ì •ìˆ˜í™”
    merged['ê±°ë˜ë‚ ì§œ'] = pd.to_datetime(merged['ê±°ë˜ë‚ ì§œ'], errors='coerce')
    merged['ê±°ë˜ë‚ ì§œ'] = merged['ê±°ë˜ë‚ ì§œ'].dt.strftime('%Y%m%d').astype(float)

    # ê±°ë˜ê¸ˆì•¡ ê³µì‹ ì ìš©
    merged['ê±°ë˜ê¸ˆì•¡'] = merged['í‰ê· ê¸ˆì•¡'] * merged['ìˆ˜ëŸ‰'] * \
        (1 + merged['GST']) + merged['ë°°ì†¡ë£Œ']

    print("ğŸ” ìµœì¢… ë³‘í•©ëœ ì»¬ëŸ¼ ëª©ë¡:", merged.columns.tolist())
    return merged


def make_features(in_columns, out_columns, is_training, data_dir='/opt/airflow/data', new_onlinesales_path=None,
                  minio_endpoint="127.0.0.1:9000", minio_access_key="admin",
                  minio_secret_key="admin", raw_data_bucket="raw-data", new_sales_bucket="new-sales"):

    save_fname = 'merged_data.pkl'

    merged_path = Path(data_dir) / save_fname
    label_path = Path(data_dir) / "label_encoder.pkl"

    client = Minio(minio_endpoint, access_key=minio_access_key,
                   secret_key=minio_secret_key, secure=False)

    # Download merged_data.pkl from MinIO if exists
    if not new_onlinesales_path and os.path.exists(merged_path):
        print(f'loading from {merged_path}')
        df = pd.read_pickle(merged_path)
    else:
        print('merging raw csv files...')
        df = merge_data(SYMBOLS, data_dir, new_onlinesales_path,
                        minio_endpoint, minio_access_key, minio_secret_key, raw_data_bucket, new_sales_bucket)
        df.to_pickle(merged_path)
        print(f'saved to {merged_path}')
        # Upload to MinIO raw_data bucket
        try:
            client.fput_object(raw_data_bucket, save_fname, merged_path)
            print(
                f"Uploaded {save_fname} to MinIO: {raw_data_bucket}/{save_fname}")
        except Exception as e:
            print(f"Failed to upload {save_fname} to MinIO: {e}")

    df.dropna(subset=out_columns, inplace=True)
    df.fillna(0, inplace=True)

    # ë¬¸ìì—´í˜• ì»¬ëŸ¼ ì¸ì½”ë”©
    label_cols = ['ì„±ë³„', 'ê³ ê°ì§€ì—­', 'ì¿ í°ì½”ë“œ', 'ì›”', 'ê³ ê°ID', 'ê±°ë˜ID', 'ì œí’ˆID', 'ì¿ í°ìƒíƒœ']
    for col in label_cols:
        if col in df.columns:
            df[col] = LabelEncoder().fit_transform(df[col].astype(str))

    # íƒ€ê²Ÿ ì¸ì½”ë”©
    if is_training:
        encoder = LabelEncoder()
        df[out_columns[0]] = encoder.fit_transform(df[out_columns[0]])
        joblib.dump(encoder, label_path)
        print(f"ğŸ”’ LabelEncoder ì €ì¥ë¨: {label_path}")
        # Upload to MinIO raw_data bucket
        try:
            client.fput_object(
                raw_data_bucket, 'label_encoder.pkl', label_path)
            print(
                f"Uploaded label_encoder.pkl to MinIO: {raw_data_bucket}/label_encoder.pkl")
        except Exception as e:
            print(f"Failed to upload label_encoder.pkl to MinIO: {e}")
    else:
        try:
            client.fget_object(
                raw_data_bucket, 'label_encoder.pkl', label_path)
            print(
                f"Downloaded label_encoder.pkl from MinIO: {raw_data_bucket}/label_encoder.pkl")
        except Exception as e:
            print(f"Failed to download label_encoder.pkl from MinIO: {e}")
        encoder = joblib.load(label_path)
        try:
            df[out_columns[0]] = encoder.transform(df[out_columns[0]])
        except ValueError as e:
            print(f"Error in LabelEncoder: {e}. Re-fitting encoder.")
            encoder = LabelEncoder()
            df[out_columns[0]] = encoder.fit_transform(df[out_columns[0]])
            joblib.dump(encoder, label_path)
            try:
                client.fput_object(
                    raw_data_bucket, 'label_encoder.pkl', label_path)
                print(
                    f"Uploaded updated label_encoder.pkl to MinIO: {raw_data_bucket}/label_encoder.pkl")
            except Exception as e:
                print(f"Failed to upload label_encoder.pkl to MinIO: {e}")
        print(f"ğŸ“¦ LabelEncoder ë¡œë“œë¨: {label_path}")

    df = df[in_columns + out_columns]
    df = df.loc[:, ~df.columns.duplicated()]

    df.columns = mapping_columns(df.columns.tolist())
    in_columns = mapping_columns(in_columns)
    out_columns = mapping_columns(out_columns)

    x_df = df[in_columns]
    y = df[out_columns].to_numpy().astype(float).squeeze()

    wide_x, deep_x = split_wide_deep_by_type(x_df)

    return ((wide_x[:-100], deep_x[:-100]), y[:-100]) if is_training else ((wide_x[-100:], deep_x[-100:]), y[-100:])


if __name__ == "__main__":
    dataset = InfoDataset(
        is_training=True,
        new_onlinesales_path="../data/Onlinesales_new.csv",
        minio_endpoint="127.0.0.1:9000",
        minio_access_key="admin",
        minio_secret_key="admin",
        raw_data_bucket="raw-data",
        new_sales_bucket="new-sales"
    )
    print(f"dataset length: {len(dataset)}")
    print(
        f"wide_x shape: {dataset.wide_x.shape}, deep_x shape: {dataset.deep_x.shape}, y shape: {dataset.y.shape}")
    print("first sample wide_x:", dataset[0][0][0])
    print("first sample deep_x:", dataset[0][0][1])
    print("first sample y:", dataset[0][1])

    # âœ… í´ë˜ìŠ¤ ìˆ˜ í™•ì¸
    import numpy as np
    unique_classes, counts = np.unique(dataset.y.numpy(), return_counts=True)
    print("\nğŸ“Š í´ë˜ìŠ¤ ê°œìˆ˜:", len(unique_classes))
    print("ğŸ§© í´ë˜ìŠ¤ ëª©ë¡:", unique_classes.tolist())
    print("ğŸ”¢ í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
    for c, n in zip(unique_classes, counts):
        print(f" - í´ë˜ìŠ¤ {int(c)}: {n}ê°œ")

    # âœ… ì…ë ¥ í”¼ì²˜ êµ¬ì„± í™•ì¸
    in_columns = [
        'ê³ ê°ID', 'ê±°ë˜ID', 'ê±°ë˜ë‚ ì§œ', 'ì œí’ˆID', 'ì œí’ˆì¹´í…Œê³ ë¦¬', 'ìˆ˜ëŸ‰', 'í‰ê· ê¸ˆì•¡', 'ë°°ì†¡ë£Œ', 'ì¿ í°ìƒíƒœ',
        'ì„±ë³„', 'ê³ ê°ì§€ì—­', 'ê°€ì…ê¸°ê°„', 'GST', 'ì›”', 'ì¿ í°ì½”ë“œ', 'í• ì¸ìœ¨', 'ê±°ë˜ê¸ˆì•¡']
    in_columns = mapping_columns(in_columns)
    print("\nğŸ§¾ ì…ë ¥ í”¼ì²˜ ëª©ë¡:")
    for idx, col in enumerate(in_columns):
        print(f"[{idx}] {col}")
